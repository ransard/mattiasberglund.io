<!DOCTYPE html>
<html lang="en"><head><title>Captioning Datasets</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;family=IBM Plex Mono:wght@400;600&amp;display=swap"/><link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin="anonymous"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="Quartz 4"/><meta property="og:title" content="Captioning Datasets"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Captioning Datasets"/><meta name="twitter:description" content="In the spirit of how open the various SD communities are in sharing their models, processes, and everything else, I thought I would write something up based on my knowledge and experience so far in an area that I think doesn’t get enough attention: captioning datasets for training purposes."/><meta property="og:description" content="In the spirit of how open the various SD communities are in sharing their models, processes, and everything else, I thought I would write something up based on my knowledge and experience so far in an area that I think doesn’t get enough attention: captioning datasets for training purposes."/><meta property="og:image:alt" content="In the spirit of how open the various SD communities are in sharing their models, processes, and everything else, I thought I would write something up based on my knowledge and experience so far in an area that I think doesn’t get enough attention: captioning datasets for training purposes."/><meta property="twitter:domain" content="quartz.jzhao.xyz"/><meta property="og:url" content="https://quartz.jzhao.xyz/3-Resources/Stable-Diffusion-1/Captioning-Datasets"/><meta property="twitter:url" content="https://quartz.jzhao.xyz/3-Resources/Stable-Diffusion-1/Captioning-Datasets"/><link rel="icon" href="../../static/icon.png"/><meta name="description" content="In the spirit of how open the various SD communities are in sharing their models, processes, and everything else, I thought I would write something up based on my knowledge and experience so far in an area that I think doesn’t get enough attention: captioning datasets for training purposes."/><meta name="generator" content="Quartz"/><link href="../../index.css" rel="stylesheet" type="text/css" spa-preserve/><style>.expand-button {
  position: absolute;
  display: flex;
  float: right;
  padding: 0.4rem;
  margin: 0.3rem;
  right: 0;
  color: var(--gray);
  border-color: var(--dark);
  background-color: var(--light);
  border: 1px solid;
  border-radius: 5px;
  opacity: 0;
  transition: 0.2s;
}
.expand-button > svg {
  fill: var(--light);
  filter: contrast(0.3);
}
.expand-button:hover {
  cursor: pointer;
  border-color: var(--secondary);
}
.expand-button:focus {
  outline: 0;
}

pre:hover > .expand-button {
  opacity: 1;
  transition: 0.2s;
}

#mermaid-container {
  position: fixed;
  contain: layout;
  z-index: 999;
  left: 0;
  top: 0;
  width: 100vw;
  height: 100vh;
  overflow: hidden;
  display: none;
  backdrop-filter: blur(4px);
  background: rgba(0, 0, 0, 0.5);
}
#mermaid-container.active {
  display: inline-block;
}
#mermaid-container > #mermaid-space {
  border: 1px solid var(--lightgray);
  background-color: var(--light);
  border-radius: 5px;
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  height: 80vh;
  width: 80vw;
  overflow: hidden;
}
#mermaid-container > #mermaid-space > .mermaid-content {
  padding: 2rem;
  position: relative;
  transform-origin: 0 0;
  transition: transform 0.1s ease;
  overflow: visible;
  min-height: 200px;
  min-width: 200px;
}
#mermaid-container > #mermaid-space > .mermaid-content pre {
  margin: 0;
  border: none;
}
#mermaid-container > #mermaid-space > .mermaid-content svg {
  max-width: none;
  height: auto;
}
#mermaid-container > #mermaid-space > .mermaid-controls {
  position: absolute;
  bottom: 20px;
  right: 20px;
  display: flex;
  gap: 8px;
  padding: 8px;
  background: var(--light);
  border: 1px solid var(--lightgray);
  border-radius: 6px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
  z-index: 2;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  padding: 0;
  border: 1px solid var(--lightgray);
  background: var(--light);
  color: var(--dark);
  border-radius: 4px;
  cursor: pointer;
  font-size: 16px;
  font-family: var(--bodyFont);
  transition: all 0.2s ease;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:hover {
  background: var(--lightgray);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:active {
  transform: translateY(1px);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:nth-child(2) {
  width: auto;
  padding: 0 12px;
  font-size: 14px;
}
/*# sourceMappingURL=data:application/json;charset=utf-8;base64,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 */</style><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="../../prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("../../static/contentIndex.json").then(data => data.json())</script><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="https://quartz.jzhao.xyz/index.xml"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image" content="https://quartz.jzhao.xyz/3-Resources/Stable-Diffusion-1/Captioning-Datasets-og-image.webp"/><meta property="og:image:url" content="https://quartz.jzhao.xyz/3-Resources/Stable-Diffusion-1/Captioning-Datasets-og-image.webp"/><meta name="twitter:image" content="https://quartz.jzhao.xyz/3-Resources/Stable-Diffusion-1/Captioning-Datasets-og-image.webp"/><meta property="og:image:type" content="image/.webp"/></head><body data-slug="3-Resources/Stable-Diffusion-1/Captioning-Datasets"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href="../..">Quartz 4</a></h2><div class="spacer mobile-only"></div><div style="display: flex; flex-direction: row; flex-wrap: nowrap; gap: 1rem;"><div style="flex-grow: 1; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><div class="search"><button class="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div class="search-container"><div class="search-space"><input autocomplete="off" class="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div class="search-layout" data-preview="true"></div></div></div></div></div><div style="flex-grow: 0; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><button class="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div></div><div class="explorer" data-behavior="link" data-collapsed="collapsed" data-savestate="true" data-data-fns="{&quot;order&quot;:[&quot;filter&quot;,&quot;map&quot;,&quot;sort&quot;],&quot;sortFn&quot;:&quot;(a,b)=>!a.isFolder&amp;&amp;!b.isFolder||a.isFolder&amp;&amp;b.isFolder?a.displayName.localeCompare(b.displayName,void 0,{numeric:!0,sensitivity:\&quot;base\&quot;}):!a.isFolder&amp;&amp;b.isFolder?1:-1&quot;,&quot;filterFn&quot;:&quot;node=>node.slugSegment!==\&quot;tags\&quot;&quot;,&quot;mapFn&quot;:&quot;node=>node&quot;}"><button type="button" class="explorer-toggle mobile-explorer hide-until-loaded" data-mobile="true" aria-controls="explorer-content"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><button type="button" class="title-button explorer-toggle desktop-explorer" data-mobile="false" aria-expanded="true"><h2>Explorer</h2><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div class="explorer-content" aria-expanded="false"><ul class="explorer-ul overflow" id="list-0"><li class="overflow-end"></li></ul></div><template id="template-file"><li><a href="#"></a></li></template><template id="template-folder"><li><div class="folder-container"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="folder-icon"><polyline points="6 9 12 15 18 9"></polyline></svg><div><button class="folder-button"><span class="folder-title"></span></button></div></div><div class="folder-outer"><ul class="content"></ul></div></li></template></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../3-Resources/">3 Resources</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../3-Resources/Stable-Diffusion-1/">Stable Diffusion 1</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>Captioning Datasets</a></div></nav><h1 class="article-title">Captioning Datasets</h1><p show-comma="true" class="content-meta"><time datetime="2024-06-15T21:57:31.183Z">Jun 15, 2024</time><span>18 min read</span></p></div></div><article class="popover-hint"><p>In the spirit of how open the various SD communities are in sharing their models, processes, and everything else, I thought I would write something up based on my knowledge and experience so far in an area that I think doesn’t get enough attention: captioning datasets for training purposes.</p>
<h1 id="disclaimer">DISCLAIMER<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#disclaimer" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>I am not an expert in this field, this is simply a mix of what has worked for me, what I’ve learned from others, my understanding of the underlying processes, and the knowledge I’ve gained from working with other types of datasets.</p>
<p>I have found this workflow to be reasonably efficient when manually captioning a dataset considering the resulting quality of the captions compared to automated captioning. But be warned, if you are looking to caption hundreds of photos, <em>it’s still gonna take some time.</em> To be clear, that means I am saying this method is not good for captioning truly large datasets with tens of thousands of images. Unless you are a masochist.</p>
<p>Sometimes I say “tag” and sometimes “caption”. I was going to go through and fix it all, but I had captioning to do, so maybe I will make it uniform later.</p>
<p>I do not consider this document “finished”. There is so much to learn, and the AI space is moving so fast, that it will likely never be finished. However, I will try to expand and alter this document as necessary.</p>
<p>My experience has primarily been with LoRA training, but some of the aspects here are applicable to all types of training.</p>
<h1 id="who-is-this-document-for">WHO IS THIS DOCUMENT FOR<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#who-is-this-document-for" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>I hope this document can be helpful to anyone who is somewhat seriously interested in training their own models in Stable Diffusion using their own datasets. If your goal is to quickly teach your face to a model, there are much better guides available which will have you up and running in a flash. But if your goal is to go a bit deeper, explore training in more depth, perhaps you can add this document to your resources.</p>
<h1 id="dataset">DATASET<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#dataset" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>Obtaining a good dataset is talked about extensively elsewhere, so I’ve only included the most important parts:</p>
<ul>
<li>
<p>high quality input means high quality output</p>
</li>
<li>
<p>more quantity and more variety is better</p>
</li>
<li>
<p>If you are forced to choose between quality and quantity, quality always wins.</p>
</li>
<li>
<p>Upscale as a last resort, avoid it if possible. When I am forced to upscale, I use LDSR via Automatic1111.</p>
</li>
</ul>
<h1 id="preparation">PREPARATION<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#preparation" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>Depending on how and what you are training, you may need to crop the photos to a specific width and height. Other types of training will bucket images into various sizes and do not require cropping. Look into what is required for the method of training you are doing, the model you are training on, and the program you are using to train your model with.</p>
<h1 id="captioning--general-notes">CAPTIONING – GENERAL NOTES<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#captioning--general-notes" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>The following recommendations are based on my experiments, my background work with other datasets, reading subject-matter papers, and borrowing from other successful approaches.</p>
<p><strong>Avoid automated captioning, for now.</strong></p>
<ul>
<li>
<p>BLIP and deepbooru are exciting, but I think it is a bit early for them yet.</p>
</li>
<li>
<p>I often find mistakes and extremely repetitive captions, which take awhile to clean up.</p>
</li>
<li>
<p>They struggle with context and with relative importance.</p>
</li>
<li>
<p>I think it is faster to manually caption, rather than fix mistakes that BLIP/deepbooru made <em>and</em> still have to manually caption.</p>
</li>
</ul>
<p><strong>Caption in the same manner you prompt.</strong></p>
<ul>
<li>
<p>Captioning and prompting are related.</p>
</li>
<li>
<p>Recognize how you typically prompt. Verbose sentences? Short descriptions? Vague? Detailed?</p>
</li>
<li>
<p>Caption in a similar style and verbosity as you tend to when prompting.</p>
</li>
</ul>
<p><strong>Follow a set structure per concept.</strong></p>
<ul>
<li>
<p>Following a structure makes the process easier on you, and although I have no objective evidence, my intuition says that using a consistent structure to describe your dataset will benefit the learning process.</p>
</li>
<li>
<p>You might have a structure you use for photographs and another structure you use for illustrations. But try to avoid mixing and matching structures when captioning a single dataset.</p>
</li>
<li>
<p>I have explained the structure I generally use below, which can be used as an example.</p>
</li>
</ul>
<p><strong>Captions are like variables you can use in your prompts.</strong></p>
<ul>
<li>Everything you describe in a caption can be thought of as a variable that you can play with in your prompt. This has two implications:</li>
</ul>
<ol>
<li>
<p>You want to describe as much detail as you can about anything that isn’t the concept you are trying to implicitly teach. <em><strong>In other words, describe everything that you want to become a variable.</strong></em> <code>Example: If you are teaching a specific face but want to be able to change the hair color, you should describe the hair color in each image so that “hair color” becomes one of your variables.</code></p>
</li>
<li>
<p>You don’t want to describe anything (beyond a class level description) that you want to be implicitly taught. <em><strong>In other words, the thing you are trying to teach shouldn’t become a variable.</strong></em> <code>Example: If you are teaching a specific face, you should not describe that it has a big nose. You don’t want the nose size to be variable, because then it isn’t that specific face anymore.However, you can still caption “face” if you want to, which provides context to the model you are training. This does have some implications described in the following point.</code></p>
</li>
</ol>
<p><strong>Leveraging classes as tags</strong></p>
<ul>
<li>There are two concepts here.</li>
</ul>
<ol>
<li>
<p>Using generic class tags will bias that entire class towards your training data. This may or may not be desired depending on what your goals are.</p>
</li>
<li>
<p>Using generic class tags provides context to the learning process. Conceptually, it is easier to learn what a “face” is when the model already has a reasonable approximation of “face”.</p>
</li>
</ol>
<ul>
<li>
<p>If you want to bias the entire class of your model towards your training images, use broad class tags rather than specific tags. <code>Example: If you want to teach your model that every man should look like Brad Pitt, your captions should contain the tag “man” but should not be more specific than that. This influences your model to produce a Brad Pitt looking man whenever you use the word “man” in your prompt. This also allows your model to draw on and leverage what it already knows about the concept of “man” while it is training.</code></p>
</li>
<li>
<p>If you want to reduce the impact of your training on the entire class, include specific tags and de-emphasize class tags. <code>Example: If you want to teach your model that only “ohwxman” should look like Brad Pitt, and you don't want every &quot;man&quot; to look like Brad Pitt you would not use &quot;man&quot; as a tag, only tagging it with “ohwxman”. This reduces the impact of your training images on the tag “man”, and strongly associates your training images with “ohwxman”. Your model will draw on what it knows about “ohwxman”, which is practically nothing *see note*, thus building up knowledge almost solely from your training images which creates a very strong association.</code></p>
</li>
<li>
<p><em>NOTE</em> This is simplified for the sake of understanding. This would actually be tokenized into two tokens, “ohwx” and “man”, but these tokens would be strongly correlated for training purposes, which should still reduce the impact on the overall class of “man” when compared to training with “man” as a token in the caption. The math it all is quite complex and well beyond the scope here.</p>
</li>
</ul>
<p><strong>Consistent Captioning</strong></p>
<ul>
<li>
<p>Use consistent captions across all of your training. This will help you better consistently invoke your concept when prompting. I use a program to aid me with this, ensuring that I always use the same captions.</p>
</li>
<li>
<p>Using inconsistent tags across your dataset is going to make the concept you are trying to teach harder for SD to grasp as you are essentially forcing it to learn both the concept and the different phrasings for that concept. It’s much better to have it just learn the concept under a single term.</p>
</li>
<li>
<p><code>For example, you probably don’t want to have both “legs raised in air” and “raised legs” if you are trying to teach one single concept of a person with their legs up in the air. You want to be able to consistently invoke this pose in your prompt, so choose one way to caption it.</code></p>
</li>
</ul>
<p><strong>Avoid Repetition</strong></p>
<ul>
<li>
<p>Try to avoid repetition wherever possible. Similar to prompting, repeating words increases the weighting of those words.</p>
</li>
<li>
<p>As an example, I often find myself repeating the word “background” too much. I might have three tags that say “background” (<code>Example: simple background, white background, lamp in background</code>). Even though I want the background to have low weight, I’ve unintentionally increased the weighting quite a bit. It would be better to combine these or reword them (<code>Example: simple white background with a lamp)</code>.</p>
</li>
</ul>
<p><strong>Take Note of Ordering</strong></p>
<ul>
<li>
<p>Again, just like with prompting, order matters for relative weighting of tags.</p>
</li>
<li>
<p>Having a specific structure/order that you generally use for captions can help you maintain relative weightings of tags between images in your dataset, which should be beneficial to the training process.</p>
</li>
<li>
<p>Having a standardized ordering makes the whole captioning process faster as you become familiar with captioning in that structure.</p>
</li>
</ul>
<p><strong>Use Your Models Existing Knowledge to Your Advantage</strong></p>
<ul>
<li>
<p>Your model already produces decent results and reasonably understands what you are prompting. Take advantage of that by captioning with words that already work well in your prompts.</p>
</li>
<li>
<p>You want to use descriptive words, but if you use words that are too obscure/niche, you likely can’t leverage much of the existing knowledge. <code>Example: you could say &quot;sarcrastic&quot; or you could say &quot;mordacious&quot;. SD has some idea of what &quot;sarcastic&quot; conveys, but it likely has no clue what &quot;mordacious&quot; is.</code></p>
</li>
<li>
<p>You can also look at this from the opposite perspective. If you were trying to teach the concept of “mordacious”, you might have a dataset full of images that convey “sarcrastic” and caption them with both the tags “sarcastic” and “mordacious” side by side (so that they are close in relative weighting).</p>
</li>
</ul>
<h1 id="captioning--structure">CAPTIONING – STRUCTURE<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#captioning--structure" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>I use this mainly for people / characters, so it might not be quite as applicable to something like fantasy landscapes, but perhaps it can give some inspiration.</p>
<p>I want to emphasize again that I am not saying this is the only or best way to caption. This is just how I have found success with my own captions on my own datasets. My goal is simply to share what I do and why, and you are free to take as much or little inspiration from it as you want.</p>
<p><strong>General format</strong></p>
<ul>
<li><globals> &lt;Type/Perspective/“Of a…”> <action words> <subject descriptions> <notable details> &lt;Background/Location> <loose associations></loose></notable></subject></action></globals></li>
</ul>
<p><strong>Globals</strong></p>
<ul>
<li>This is where I would stick a rare token (e.g. “ohwx”) that I want heavily associated with the concept I am training, or anything that is both important to the training and uniform across the dataset <code>Examples: man, woman, anime</code></li>
</ul>
<p><strong>Type/Perspective/“of a…”</strong></p>
<ul>
<li>
<p>Broad descriptions of the image to supply context. I usually do this in “layers”.</p>
</li>
<li>
<p>What is it? <code>Examples: photograph, illustration, drawing, portrait, render, anime.</code></p>
</li>
<li>
<p>Of a… <code>Examples: woman, man, mountain, trees, forest, fantasy scene, cityscape</code></p>
</li>
<li>
<p>What type of X is it (x = choice above)? <code>Examples: full body, close up, cowboy shot, cropped, filtered, black and white, landscape, 80s style</code></p>
</li>
<li>
<p>What perspective is X from? <code>Examples: from above, from below, from front, from behind, from side, forced perspective, tilt-shifted, depth of field</code></p>
</li>
</ul>
<p><strong>Action Words</strong></p>
<ul>
<li>
<p>Descriptions of what the main subject is doing or what is happening to the main subject, or general verbs that are applicable to the concept in the image. Describe in as much detail as possible, with a combination of as many verbs as you want.</p>
</li>
<li>
<p>The goal is to make all the actions, poses, and whatever else active that is happening into variables (as described in point 3 of “Captioning – General”) so that, hopefully, SD is better able to learn the main concept in a general sense rather than only learning the main concept doing specific actions.</p>
</li>
<li>
<p><code>Using a person as an example: standing, sitting, leaning, arms above head, walking, running, jumping, one arm up, one leg out, elbows bent, posing, kneeling, stretching, arms in front, knee bent, lying down, looking away, looking up, looking at viewer</code></p>
</li>
<li>
<p><code>Using a flower as an example: wilting, growing, blooming, decaying, blossoming</code></p>
</li>
</ul>
<p><strong>Subject Descriptions</strong></p>
<ul>
<li>
<p>As much description about the subject as possible, without describing the main concept you are trying to teach. Once again, think of this as picking out everything that you want to be a variable in your prompt.</p>
</li>
<li>
<p><code>Using a person as an example: white hat, blue shirt, silver necklace, sunglasses, pink shoes, blonde hair, silver bracelet, green jacket, large backpack</code></p>
</li>
<li>
<p><code>Using a flower as an example: pink petals, green leaves, tall, straight, thorny, round leaves</code></p>
</li>
</ul>
<p><strong>Notable Details</strong></p>
<ul>
<li>
<p>I use this as a sort of catch-all for anything that I don’t think is quite “background” (or something that is background but I want to emphasize) but also isn’t the main subject.</p>
</li>
<li>
<p>Normally the part of the caption going in this spot is unique to one or just a few training images.</p>
</li>
<li>
<p>I predominately use short captions in Danbooru-style, but if I need to describe something more complex I put it here.</p>
</li>
<li>
<p><code>For example, in a photo at a beach I might put “yellow and blue striped umbrella partially open in foreground”.</code></p>
</li>
<li>
<p><code>For example, in a portrait I might put “he is holding a cellphone to his ear”.</code></p>
</li>
</ul>
<p><strong>Background / Location</strong></p>
<ul>
<li>
<p>Fairly self-explanatory. Be as descriptive as possible about what is happening in the images background. I tend to do this in a few “layers” as well, narrowing down to specifics, which helps when captioning several photos.</p>
</li>
<li>
<p><code>For example, for a beach photo I might put (separated by the three “layers”):</code></p>
</li>
</ul>
<ol>
<li>
<p><code>Outdoors, beach, sand, water, shore, sunset</code></p>
</li>
<li>
<p><code>Small waves, ships out at sea, sandcastle, towels</code></p>
</li>
<li>
<p><code>the ships are red and white, the sandcastle has a moat around it, the towels are red with yellow stripes</code></p>
</li>
</ol>
<p><strong>Loose Associations</strong></p>
<ul>
<li>
<p>This is where I put any final loose associations I have with the image.</p>
</li>
<li>
<p>This could be anything that pops up in my head, usually “feelings” that I feel when looking at the image or concepts I feel are portrayed, really anything goes here as long <em>as it exists in the image</em>.</p>
</li>
<li>
<p>Keep in mind this is for <em>loose</em> associations. If the image is <em>very obviously</em> portraying some feeling, you may want it tagged closer to the start of the caption for higher weighting.</p>
</li>
<li>
<p><code>For example: happy, sad, joyous, hopeful, lonely, sombre</code></p>
</li>
</ul>
<h1 id="the-booru-dataset-tag-manager">THE BOORU DATASET TAG MANAGER<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#the-booru-dataset-tag-manager" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>You’ve got a dataset. You’ve decided on a structure. You’re ready to start captioning. Now it’s time for the magic part of the workflow: <a href="https://github.com/starik222/BooruDatasetTagManager" class="external">BooruDatasetTagManager<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> (BDTM). This handy piece of software will do two extremely important things for us which greatly speeds up the workflow:</p>
<ol>
<li>
<p>Tags are preloaded in *\tags\list.tag, which can be edited. This gives us auto-complete for common tags, allows us to double-click common tags so we don’t need to type it out, etc.</p>
</li>
<li>
<p>It enables you to easily be consistent with your captioning by displaying already-used captions so that you can easily add it to an image without typing it out.</p>
</li>
</ol>
<p>As an added bonus, it helps when you’re forgetful. Sometimes I forget that standing with most of your weight on one foot (but with both feet on the ground) is called contrapposto. But I have it saved as a tag, and usually remember it starts as “contra”. Thankfully auto-complete is there to save the day. Seriously, having all of these tags at your fingertips is a huge difference from trying to remember a bunch of tags or having booru sites open in other tabs.</p>
<h1 id="the-process">THE PROCESS<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#the-process" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<ol>
<li>
<p>Place all of your images in a folder and then navigate there in the BDTM UI, selecting the folder with your images.</p>
</li>
<li>
<p>At the top, press “View” and then “Show preview” to see the selected image.</p>
</li>
<li>
<p>If you have any globally applicable tags, add them on the right side of the UI. You can select where these global tags appear (top, bottom, or at a specific position in the list).</p>
</li>
<li>
<p>Select your image on the left and begin adding tags, remembering to follow you structure as best as possible. As you type, the tags will show auto-complete options from the list.tag file which you can select, or you can type in your own custom ones.</p>
</li>
<li>
<p>Each tag you have used anywhere in that dataset will show on the right side (under “All tags”). You can double-click a tag from the “All tags” section to apply it to the currently selected image, saving tons of time and ensuring tag consistency across your dataset</p>
</li>
<li>
<p>Once all of your images are tagged, go back to the start and do it again. This time look at your tags and make sure they are ordered appropriately according to the weighting you want (you can drag them to reorder if necessary), make sure they follow your structure, check for missing tags, etc.</p>
</li>
</ol>
<p>And that’s it. I patiently look at every image and add any tags I think are applicable, aiming to have at least one to two tags in each of the categories of my prompt structure. I usually have between 8 and 20 tags per image, though sometimes I might have even more.</p>
<p>Over time, I have edited the provided list.tag file removing many of the tags I’ll never use and adding a bunch of tags that I use frequently, making the whole process even easier.</p>
<h1 id="full-example-of-a-single-image">FULL EXAMPLE OF A SINGLE IMAGE<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#full-example-of-a-single-image" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>This is an example of how I would caption a single image I picked off of safebooru. <em>We will assume that I want to train the style of this image and associate it with the tag “ohwxStyle”, and we will assume that I have many images in this style within my dataset.</em></p>
<p>Sample Image: <a href="https://safebooru.org/index.php?page=post&amp;s=view&amp;id=3887414" class="external">https://safebooru.org/index.php?page=post&amp;s=view&amp;id=3887414<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
<ul>
<li>
<p>Globals: ohwxStyle</p>
</li>
<li>
<p>Type/Perspective/Of a: anime, drawing, of a young woman, full body shot, from side</p>
</li>
<li>
<p>Action words: sitting, looking at viewer, smiling, head tilt, holding a phone, eyes closed</p>
</li>
<li>
<p>Subject description: short brown hair, pale pink dress with dark edges, stuffed animal in lap, brown slippers</p>
</li>
<li>
<p>Notable details: sunlight through windows as lighting source</p>
</li>
<li>
<p>Background/location: brown couch, red patterned fabric on couch, wooden floor, white water-stained paint on walls, refrigerator in background, coffee machine sitting on a countertop, table in front of couch, bananas and coffee pot on table, white board on wall, clock on wall, stuffed animal chicken on floor</p>
</li>
<li>
<p>Loose associations: dreary environment</p>
</li>
</ul>
<p>All together: ohwxStyle, anime, drawing, of a young woman, full body shot, from side, sitting, looking at viewer, smiling, head tilt, holding a phone, eyes closed, short brown hair, pale pink dress with dark edges, stuffed animal in lap, brown slippers, sunlight through windows as lighting source, brown couch, red patterned fabric on couch, wooden floor, white water-stained paint on walls, refrigerator in background, coffee machine sitting on a countertop, table in front of couch, bananas and coffee pot on table, white board on wall, clock on wall, stuffed animal chicken on floor, dreary environment</p>
<p>The best part is, I can set all of those “global” ones in BDTM to apply to all of my images. I’ve now also got all of those tags ready just a double-click away, so if my next image is also a full body shot, from the side, sitting… I just double-click it. Much easier than typing it out again.</p>
<h1 id="train">TRAIN<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#train" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<p>Time to start training! I don’t have much to write here other than experiment. There is no golden number of steps or guaranteed results when it comes to training. That’s why it’s fun to experiment. And now you can experiment knowing that you have an extremely high quality dataset, allowing you to really hone-in on the appropriate training settings.</p>
<h1 id="misc-thoughts-and-references">MISC THOUGHTS AND REFERENCES<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#misc-thoughts-and-references" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h1>
<ul>
<li>
<p>I always try to remind myself that we are just gently guiding the learning process, not controlling it. Your captions <em>help</em> <em>point the learning process in the right direction</em>, but the captions are not absolute. Inferences will be made on things in the image that weren’t captioned, associations will be made between tags and parts of the image you didn’t intend, etc. Try to guide, but trust in the training and the quality of your images as well.</p>
</li>
<li>
<p>Danbooru/safebooru tags are great. I mean, there’s a lot of trash ones that hold no meaning, but take a look at the Danbooru wiki for tag group “Posture” as an example. Dozens of specific words for different arm positions, leg positions, etc. You might just find that one specific word you’ve been searching for that describes the style/pose/lighting/whatever by crawling through the danbooru tags and wiki. Maybe you’ve always wanted someone posing with that ballerina style foot where the toes are pointed downwards. Well it’s called plantar flexion; thanks danbooru tags.</p>
</li>
</ul>
<p><em>References</em>
<a href="https://www.reddit.com/r/StableDiffusion/comments/118spz6/captioning_datasets_for_training_purposes/" class="external">https://www.reddit.com/r/StableDiffusion/comments/118spz6/captioning_datasets_for_training_purposes/<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div class="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false,&quot;enableRadial&quot;:false}"></div><button class="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div class="global-graph-outer"><div class="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.2,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true,&quot;enableRadial&quot;:true}"></div></div></div><div class="toc desktop-only"><button type="button" class="toc-header" aria-controls="toc-content" aria-expanded="true"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><ul class="toc-content overflow" id="list-1"><li class="depth-0"><a href="#disclaimer" data-for="disclaimer">DISCLAIMER</a></li><li class="depth-0"><a href="#who-is-this-document-for" data-for="who-is-this-document-for">WHO IS THIS DOCUMENT FOR</a></li><li class="depth-0"><a href="#dataset" data-for="dataset">DATASET</a></li><li class="depth-0"><a href="#preparation" data-for="preparation">PREPARATION</a></li><li class="depth-0"><a href="#captioning--general-notes" data-for="captioning--general-notes">CAPTIONING – GENERAL NOTES</a></li><li class="depth-0"><a href="#captioning--structure" data-for="captioning--structure">CAPTIONING – STRUCTURE</a></li><li class="depth-0"><a href="#the-booru-dataset-tag-manager" data-for="the-booru-dataset-tag-manager">THE BOORU DATASET TAG MANAGER</a></li><li class="depth-0"><a href="#the-process" data-for="the-process">THE PROCESS</a></li><li class="depth-0"><a href="#full-example-of-a-single-image" data-for="full-example-of-a-single-image">FULL EXAMPLE OF A SINGLE IMAGE</a></li><li class="depth-0"><a href="#train" data-for="train">TRAIN</a></li><li class="depth-0"><a href="#misc-thoughts-and-references" data-for="misc-thoughts-and-references">MISC THOUGHTS AND REFERENCES</a></li><li class="overflow-end"></li></ul></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.5.0</a> © 2025</p><ul><li><a href="https://github.com/jackyzha0/quartz">GitHub</a></li><li><a href="https://discord.gg/cRFFHYye7t">Discord Community</a></li></ul></footer></div></div></body><script type="application/javascript">function o(){let t=this.parentElement;t.classList.toggle("is-collapsed");let e=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=e+"px";let c=t,l=t.parentElement;for(;l;){if(!l.classList.contains("callout"))return;let i=l.classList.contains("is-collapsed")?l.scrollHeight:l.scrollHeight+c.scrollHeight;l.style.maxHeight=i+"px",c=l,l=l.parentElement}}function n(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let e=s.firstElementChild;if(!e)continue;e.addEventListener("click",o),window.addCleanup(()=>e.removeEventListener("click",o));let l=s.classList.contains("is-collapsed")?e.scrollHeight:s.scrollHeight;s.style.maxHeight=l+"px"}}document.addEventListener("nav",n);
</script><script type="module">function f(i,e){if(!i)return;function r(o){o.target===this&&(o.preventDefault(),o.stopPropagation(),e())}function t(o){o.key.startsWith("Esc")&&(o.preventDefault(),e())}i?.addEventListener("click",r),window.addCleanup(()=>i?.removeEventListener("click",r)),document.addEventListener("keydown",t),window.addCleanup(()=>document.removeEventListener("keydown",t))}function y(i){for(;i.firstChild;)i.removeChild(i.firstChild)}var h=class{constructor(e,r){this.container=e;this.content=r;this.setupEventListeners(),this.setupNavigationControls(),this.resetTransform()}isDragging=!1;startPan={x:0,y:0};currentPan={x:0,y:0};scale=1;MIN_SCALE=.5;MAX_SCALE=3;cleanups=[];setupEventListeners(){let e=this.onMouseDown.bind(this),r=this.onMouseMove.bind(this),t=this.onMouseUp.bind(this),o=this.resetTransform.bind(this);this.container.addEventListener("mousedown",e),document.addEventListener("mousemove",r),document.addEventListener("mouseup",t),window.addEventListener("resize",o),this.cleanups.push(()=>this.container.removeEventListener("mousedown",e),()=>document.removeEventListener("mousemove",r),()=>document.removeEventListener("mouseup",t),()=>window.removeEventListener("resize",o))}cleanup(){for(let e of this.cleanups)e()}setupNavigationControls(){let e=document.createElement("div");e.className="mermaid-controls";let r=this.createButton("+",()=>this.zoom(.1)),t=this.createButton("-",()=>this.zoom(-.1)),o=this.createButton("Reset",()=>this.resetTransform());e.appendChild(t),e.appendChild(o),e.appendChild(r),this.container.appendChild(e)}createButton(e,r){let t=document.createElement("button");return t.textContent=e,t.className="mermaid-control-button",t.addEventListener("click",r),window.addCleanup(()=>t.removeEventListener("click",r)),t}onMouseDown(e){e.button===0&&(this.isDragging=!0,this.startPan={x:e.clientX-this.currentPan.x,y:e.clientY-this.currentPan.y},this.container.style.cursor="grabbing")}onMouseMove(e){this.isDragging&&(e.preventDefault(),this.currentPan={x:e.clientX-this.startPan.x,y:e.clientY-this.startPan.y},this.updateTransform())}onMouseUp(){this.isDragging=!1,this.container.style.cursor="grab"}zoom(e){let r=Math.min(Math.max(this.scale+e,this.MIN_SCALE),this.MAX_SCALE),t=this.content.getBoundingClientRect(),o=t.width/2,n=t.height/2,c=r-this.scale;this.currentPan.x-=o*c,this.currentPan.y-=n*c,this.scale=r,this.updateTransform()}updateTransform(){this.content.style.transform=`translate(${this.currentPan.x}px, ${this.currentPan.y}px) scale(${this.scale})`}resetTransform(){this.scale=1;let e=this.content.querySelector("svg");this.currentPan={x:e.getBoundingClientRect().width/2,y:e.getBoundingClientRect().height/2},this.updateTransform()}},C=["--secondary","--tertiary","--gray","--light","--lightgray","--highlight","--dark","--darkgray","--codeFont"],E;document.addEventListener("nav",async()=>{let e=document.querySelector(".center").querySelectorAll("code.mermaid");if(e.length===0)return;E||=await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.esm.min.mjs");let r=E.default,t=new WeakMap;for(let n of e)t.set(n,n.innerText);async function o(){for(let s of e){s.removeAttribute("data-processed");let a=t.get(s);a&&(s.innerHTML=a)}let n=C.reduce((s,a)=>(s[a]=window.getComputedStyle(document.documentElement).getPropertyValue(a),s),{}),c=document.documentElement.getAttribute("saved-theme")==="dark";r.initialize({startOnLoad:!1,securityLevel:"loose",theme:c?"dark":"base",themeVariables:{fontFamily:n["--codeFont"],primaryColor:n["--light"],primaryTextColor:n["--darkgray"],primaryBorderColor:n["--tertiary"],lineColor:n["--darkgray"],secondaryColor:n["--secondary"],tertiaryColor:n["--tertiary"],clusterBkg:n["--light"],edgeLabelBackground:n["--highlight"]}}),await r.run({nodes:e})}await o(),document.addEventListener("themechange",o),window.addCleanup(()=>document.removeEventListener("themechange",o));for(let n=0;n<e.length;n++){let v=function(){let g=l.querySelector("#mermaid-space"),m=l.querySelector(".mermaid-content");if(!m)return;y(m);let w=c.querySelector("svg").cloneNode(!0);m.appendChild(w),l.classList.add("active"),g.style.cursor="grab",u=new h(g,m)},M=function(){l.classList.remove("active"),u?.cleanup(),u=null},c=e[n],s=c.parentElement,a=s.querySelector(".clipboard-button"),d=s.querySelector(".expand-button"),p=window.getComputedStyle(a),L=a.offsetWidth+parseFloat(p.marginLeft||"0")+parseFloat(p.marginRight||"0");d.style.right=`calc(${L}px + 0.3rem)`,s.prepend(d);let l=s.querySelector("#mermaid-container");if(!l)return;let u=null;d.addEventListener("click",v),f(l,M),window.addCleanup(()=>{u?.cleanup(),d.removeEventListener("click",v)})}});
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="../../postscript.js" type="module"></script></html>